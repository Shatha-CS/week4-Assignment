{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rGZPvRADNx4i",
        "outputId": "c277a26c-b508-456a-863d-2ba57bc6fa60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnYy6GIPI2M3",
        "outputId": "5409303c-97e6-407c-ae06-6666855efe68"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset:"
      ],
      "metadata": {
        "id": "iHQBcVxA-yIX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4gGlOQ-4WIl",
        "outputId": "9a288d26-ba27-4bef-d953-3a27fc3c77bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'plants-type-datasets' dataset.\n",
            "Path to dataset files: /kaggle/input/plants-type-datasets\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"yudhaislamisulistya/plants-type-datasets\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_dir = \"/kaggle/input/plants-type-datasets\"\n",
        "\n",
        "print(\"Contents of dataset root:\")\n",
        "print(os.listdir(base_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-ONx67C9r-q",
        "outputId": "10a609bb-3cb2-4c74-b2e5-f612a9fbcdde"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of dataset root:\n",
            "['split_ttv_dataset_type_of_plants']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_dir = \"/kaggle/input/plants-type-datasets/split_ttv_dataset_type_of_plants\"\n",
        "\n",
        "print(\"Train / Validation / Test folders:\")\n",
        "print(os.listdir(split_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF6O0coQ-bJn",
        "outputId": "b1c34381-e637-44dd-c1b2-18c566d31b75"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train / Validation / Test folders:\n",
            "['Test_Set_Folder', 'Validation_Set_Folder', 'Train_Set_Folder']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DataLoaders:"
      ],
      "metadata": {
        "id": "4tPFKKm6A_G1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations"
      ],
      "metadata": {
        "id": "i7qnuzGlCFCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "image_size = 64\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.ToTensor(),\n",
        "])\n"
      ],
      "metadata": {
        "id": "aELkDfsdCLJv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Image Datasets"
      ],
      "metadata": {
        "id": "Wm_6PnygDqEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = \"/kaggle/input/plants-type-datasets/split_ttv_dataset_type_of_plants/Train_Set_Folder\"\n",
        "val_dir   = \"/kaggle/input/plants-type-datasets/split_ttv_dataset_type_of_plants/Validation_Set_Folder\"\n",
        "test_dir  = \"/kaggle/input/plants-type-datasets/split_ttv_dataset_type_of_plants/Test_Set_Folder\""
      ],
      "metadata": {
        "id": "dgUS4ddUnuxS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "train_dataset = ImageFolder(root=train_dir, transform=train_transforms)\n",
        "val_dataset   = ImageFolder(root=val_dir, transform=val_test_transforms)\n",
        "test_dataset  = ImageFolder(root=test_dir, transform=val_test_transforms)\n",
        "\n",
        "num_classes = len(train_dataset.classes)\n",
        "print(\"Number of classes:\", num_classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DtqYTBVCl2I",
        "outputId": "4a679113-e900-423a-d46c-15ffe9099b44"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoaders"
      ],
      "metadata": {
        "id": "3fdJsJjiDHl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"Train batches:\", len(train_loader))\n",
        "print(\"Validation batches:\", len(val_loader))\n",
        "print(\"Test batches:\", len(test_loader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Scub6JLfDBbU",
        "outputId": "05203483-f13b-41be-b0aa-a4c6ccd8680f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 375\n",
            "Validation batches: 48\n",
            "Test batches: 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convolutional Neural Network (CNN):"
      ],
      "metadata": {
        "id": "7Keh4BR9Geb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "VtEQXO6XvKA2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleCNN(num_classes=30).to(device)\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K91RdRQYHvN",
        "outputId": "31a6893a-839b-4fad-8055-8cf4dd066b8f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SimpleCNN(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): ReLU()\n",
            "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): LazyLinear(in_features=0, out_features=256, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=256, out_features=30, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Loop:"
      ],
      "metadata": {
        "id": "rb8otN5myuwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def training_loop(model, train_loader, val_loader, loss_function, optimizer, num_epochs, device):\n",
        "    model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "        train_acc = 100.0 * correct / total\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = loss_function(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss /= len(val_loader.dataset)\n",
        "        val_acc = 100.0 * correct / total\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{num_epochs}] | \"\n",
        "            f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% | \"\n",
        "            f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "97_6Pl9sy4HY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "training_loop(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    loss_function=loss_function,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=num_epochs,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OFasvrtc5Zx",
        "outputId": "de90a14a-1c4d-4068-9f20-7740f49b9399"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5] | Train Loss: 2.4872, Train Acc: 24.07% | Val Loss: 2.0081, Val Acc: 37.43%\n",
            "Epoch [2/5] | Train Loss: 1.8090, Train Acc: 42.70% | Val Loss: 1.6648, Val Acc: 45.51%\n",
            "Epoch [3/5] | Train Loss: 1.5178, Train Acc: 51.84% | Val Loss: 1.4689, Val Acc: 53.33%\n",
            "Epoch [4/5] | Train Loss: 1.3199, Train Acc: 57.53% | Val Loss: 1.3245, Val Acc: 58.61%\n",
            "Epoch [5/5] | Train Loss: 1.1663, Train Acc: 61.59% | Val Loss: 1.1375, Val Acc: 63.30%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing:"
      ],
      "metadata": {
        "id": "Z4NP2vlcFsqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, test_loader, loss_function, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = loss_function(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    test_loss = test_loss / len(test_loader.dataset)\n",
        "    test_acc = 100.0 * correct / total\n",
        "\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    return test_loss, test_acc\n"
      ],
      "metadata": {
        "id": "OJE4iE3yLgJd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = test_model(model, test_loader, loss_function, device)"
      ],
      "metadata": {
        "id": "qwr55T1KLx94",
        "outputId": "bf5a60e8-269f-45ed-d5c4-fa97f5341e94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.1420 | Test Accuracy: 63.48%\n"
          ]
        }
      ]
    }
  ]
}